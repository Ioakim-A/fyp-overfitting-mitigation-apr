# Reproducing Figures from the Experiment

This document outlines the steps required to re-create all the figures used in the report. The process involves two main phases:

1. **Data Generation** – Running Python scripts to produce CSV files.
2. **Visualisation** – Using the generated CSVs as input to visualisation scripts.

---

## Prerequisites

Before running any scripts, make sure the following are installed:

- Python >= 3.8
- Required packages (install with `pip install -r figure-replication-requirements.txt`)

## Step-by-Step Instructions

**Before completing any step, ensure your working directory is `figure_replication/scripts`**

### 0. Filter Patch Predictions (Already Done)

Some PCA tools were unable to generate predictions for the full set of patches. To allow for fairer comparisons, we find the common set of successfully classified patches across all tools and filter out patches that did not meet this criteria:

```bash
python filter_patches.py <path to results folder> <name of common file to filter e.g. 8h-deduplicated.csv>
```

For example:
```bash
python filter_patches.py ../../results/ 8h_deduplicated.csv
```

Results will be saved in the respective tool results directory and the csv will take the naming convention of the target file, appending '_filtered' e.g. 8h_deduplicated_filtered.csv

**Note that this step has already been completed.**

### Full Replication Script

We provide a script which automates the entire data processing and figure/table generation for both datasets used in this study. The usage is as follows:

```bash
# Available datasets: petke_8h (patches generated by classical APR tools over 8 hours), petke_1h (patches generated by classical APR tools over 1 hour), repairllama (patches generated by the LLM-based RepairLlama tool).
# The optional --figures argument skips the data processing steps (which can take up to an hour) and only generates the figures/tables. You should not use this argument if you are running this script for the first time and replicating the results from scratch.
python replicate_all.py {dataset} {--figures}
```


### 1. Generate CSV Files

These scripts will generate intermediate CSV files needed for plotting. The following examples are for the Petke et al. 8-hour dataset.

#### RQ3 Data

```bash
# Generate overall performance data with Random Selection metric and bootstrapping.
# Note that bootstrapping can take several minutes so if you are not interested in error bars you can exclude --bootstrap
python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --overall --bootstrap --include-wbc --wbc-p-overfit 0.50 --format csv --output ../petke8h_figures/raw_data/overall.csv 
```

The script above uses the weighted probability classifier with a 50% probability of guessing overfitting, effectively making it equivalent to Random Selection. Therefore, in the generated csv, replace its name 'WBC' to 'RS'.

```bash
# Generate MCC scores on bug-level prediction for each tool
python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --metric 'Smooth MCC' --aggregate bug --format csv --output ../petke8h_figures/raw_data/mcc_by_bug.csv
```

```bash
# Generate MCC scores on APR-tool-level prediction for each tool
python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --metric 'Smooth MCC' --aggregate approach --format csv --output ../petke8h_figures/raw_data/mcc_by_apr_tool.csv
```

```bash
# Generate F1 scores on bug-level prediction for each tool
python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --metric 'F1 Score' --aggregate bug --format csv --output ../petke8h_figures/raw_data/f1.csv
```

#### RQ4 Data

```bash
# Generate RS -85 and -95 baseline csvs
python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --aggregate bug --metric RS --confidence 85 --format csv --output ../petke_8h_figures/tables/rq4/rs85.csv

python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --aggregate bug --metric RS --confidence 95 --format csv --output ../petke_8h_figures/tables/rq4/rs95.csv

python combine_rs_columns.py ../petke_8h_figures/tables/rq4/rs85.csv ../petke_8h_figures/tables/rq4/rs95.csv ../petke_8h_figures/tables/rq4/rs-combined.csv
```

### 2. Generate Figures

These scripts use the CSVs created in the previous step to generate the final figures. The following examples are for the Petke et al. 8-hour dataset.

#### RQ3 Figures

```bash
# Generate bar charts for overall performance data
python produce_bar_chart.py --with-ci --input_csv ../petke_8h_figures/raw_data/overall.csv --output_dir ../petke_8h_figures/visualisations/rq3
```

```bash
# Generate upset plots for correct and overfitting detection
python produce_upset_diagrams.py 8h_deduplicated_filtered --input_dir ../../results --output_dir ../petke_8h_figures/visualisations/rq3
```

```bash
# Generate violin plot for MCC bug-prediction comparison and box plot for MCC project comparison, and print stats used to aid figure
python produce_violin_plots.py --input_csv ../petke_8h_figures/raw_data/mcc_by_bug.csv --output_dir ../petke_8h_figures/visualisations/rq3 --colour_by_project
```  

```bash
# Generate heatmap for APR tool - Overfitting detection tool MCC comparison
python produce_tool_apr_heatmap.py ../petke_8h_figures/raw_data/mcc_by_apr_tool.csv --output ../petke_8h_figures/visualisations/rq3/apr_tool_mcc_heatmap.png
```

```bash
# Generate bug hardness plot and regression stats
python produce_hardness_plot.py ../petke_8h_figures/raw_data/f1.csv --deciles 9 --output ../petke_8h_figures/visualisations/rq3/bug_hardness_f1.png
```

#### RQ4 Figures

```bash
# Generate RS -85 and -95 inspection tables (latex)
python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --aggregate bug --metric 'RS' --confidence 85 --format latex --output ../petke_8h_figures/tables/rq4/rs85-latex.txt

python produce_csvs_or_latex_tables.py 8h_deduplicated_filtered --aggregate bug --metric 'RS' --confidence 95 --format latex --output ../petke_8h_figures/tables/rq4/rs95-latex.txt

python create_bug_level_latex_table.py ../petke_8h_figures/tables/rq4/rs-combined.csv --output=../petke_8h_figures/tables/rq4/rs-combined.tex
```

```bash
# Generate RS -85 and -95 Median and Mean comparison stats (printed to terminal)
python produce_rsb_metrics.py ../petke_8h_figures/tables/rq4/rs85.csv

python produce_rsb_metrics.py ../petke_8h_figures/tables/rq4/rs95.csv
```

```bash
# Generate comparisons for tools vs WBC. For this, we reuse /petke_8h_figures/raw_data/overall.csv by making a copy and deleting the RS row as that is not an APR tool, and saving the result as overall-no-rs.csv. The script also takes the number of correct to overfitting patches e.g. in the case of the 8h_deduplicated_filtered dataset, that is 127:671. This also prints information about tools' confidence intervals intersecting the margin.
python produce_wbc_graphs.py ../petke_8h_figures/raw_data/overall-no-rs.csv 127 671 --out ../petke_8h_figures/visualisations/rq4/tools_vs_wbc.png
```

### 3. Filtering Results by Time
The above results produce comparisons for the 8 hour dataset. Since this contains all patches you can filter the results to e.g. patches generated in 1 hour and generate any of the above figures again.

This can be done by running for a given tool:
```bash
# Repeat for all approaches
python filter_results_by_time.py --from-hour 8 --to-hour 1 --approach LLM4PatchCorrectness
```

After filtering patch predictions by time, it is important to ensure that all tools are compared on the same set of compilable patches. In particular, Invalidator and FIXCHECK predictions include only compilable patches, while other tools may predict on additional non-compilable ones. Therefore, after filtering results by time, you must re-apply the patch filtering step using `filter_patches.py` as described in Step 0 to maintain a consistent and fair comparison across tools.