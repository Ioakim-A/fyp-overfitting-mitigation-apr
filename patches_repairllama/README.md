# Explanation about the data

## Source

We analyse the patches from the RepairLlama experiment.
This experiment executes different trials, each one with a LLM and a prompt strategies, and store the trial results in a file.
The authors of the RepairLlama conducted manual assessment, which is included in the results files.
In this experiment we analyse the results that include the manual analysis of the three authors of repairLlama: https://github.com/ASSERT-KTH/repairllama/tree/main/results/3_martin


## Patches 
There are two folders: `diff function` and `diff file`.
For the first one, `diff function` contains the diff computed from the buggy and fixed function. 
This type of diff is computed from the information present on the RepairLlama results [example](https://raw.githubusercontent.com/ASSERT-KTH/repairllama/refs/heads/main/results/3_martin/evaluation_defects4j_deepseek_base_martin.jsonl).
That is, for each row on a result file, we compute the diff from `buggy_code` and `fixed_code`fields.
The problem of this approach is that the generated diff cannot be applied to the buggy file (in order to, for example, execute dynamic analysis, such as test case execution).
The reason is that these two piece of code do no have the original file number from the files, and that do not allow to apply the patch.

The second folder `diff file` is the one we should use. It contains diff computed from the buggy and fixed files. 
Consequently, the diff contains the correct line numbers.
The context size is 3 lines of code. 
We can eventually increase it.

## Experiment to be done:

We don't have to analyze the patches from all the trials the Repairllama repository has.
We can focus on important trials. 
For example, according to RepairLlama paper (Table 2) the configuration IR4 x OR2 (RepairLLaMA) achieves the best performance. The results are in [this file](https://github.com/ASSERT-KTH/repairllama/blob/main/results/3_martin/evaluation_defects4j_repairllama_ir4_or2_martin.jsonl). 
Additionally, We can eventually analyze patches generated by other models e.g. GPT4.
